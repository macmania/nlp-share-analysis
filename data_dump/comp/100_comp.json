{"count": 19846, "results": [{"publisher": {"name": ""}, "description": "  We present an unsupervised framework for simultaneous appearance-based object\ndiscovery, detection, tracking and reconstruction using RGBD cameras and a\nrobot manipulator. The system performs dense 3D simultaneous localization and\nmapping concurrently with unsupervised object discovery. Putative objects that\nare spatially and visually coherent are manipulated by the robot to gain\nadditional motion-cues. The robot uses appearance alone, followed by structure\nand motion cues, to jointly discover, verify, learn and improve models of\nobjects. Induced motion segmentation reinforces learned models which are\nrepresented implicitly as 2D and 3D level sets to capture both shape and\nappearance. We compare three different approaches for appearance-based object\ndiscovery and find that a novel form of spatio-temporal super-pixels gives the\nhighest quality candidate object models in terms of precision and recall. Live\nexperiments with a Baxter robot demonstrate a holistic pipeline capable of\nautomatic discovery, verification, detection, tracking and reconstruction of\nunknown objects.\n", "contributors": [{"name": "Ma, Lu", "sameAs": [], "familyName": "", "additionalName": "", "givenName": "Lu", "email": ""}, {"name": "Ghafarianzadeh, Mahsa", "sameAs": [], "familyName": "Ghafarianzadeh", "additionalName": "", "givenName": "Mahsa", "email": ""}, {"name": "Coleman, Dave", "sameAs": [], "familyName": "Coleman", "additionalName": "", "givenName": "Dave", "email": ""}, {"name": "Correll, Nikolaus", "sameAs": [], "familyName": "Correll", "additionalName": "", "givenName": "Nikolaus", "email": ""}, {"name": "Sibley, Gabe", "sameAs": [], "familyName": "Sibley", "additionalName": "", "givenName": "Gabe", "email": ""}], "title": "Simultaneous Localization, Mapping, and Manipulation for Unsupervised\n  Object Discovery", "shareProperties": {"source": "arxiv_oai"}, "otherProperties": [{"name": "type", "properties": {"type": "text"}}, {"name": "format", "properties": {"format": []}}, {"name": "date", "properties": {"date": "2014-11-04"}}, {"name": "identifier", "properties": {"identifier": ["http://arxiv.org/abs/1411.0802", "oai:arXiv.org:1411.0802"]}}, {"name": "setSpec", "properties": {"setSpec": "cs"}}, {"name": "description", "properties": {"description": "  We present an unsupervised framework for simultaneous appearance-based object\ndiscovery, detection, tracking and reconstruction using RGBD cameras and a\nrobot manipulator. The system performs dense 3D simultaneous localization and\nmapping concurrently with unsupervised object discovery. Putative objects that\nare spatially and visually coherent are manipulated by the robot to gain\nadditional motion-cues. The robot uses appearance alone, followed by structure\nand motion cues, to jointly discover, verify, learn and improve models of\nobjects. Induced motion segmentation reinforces learned models which are\nrepresented implicitly as 2D and 3D level sets to capture both shape and\nappearance. We compare three different approaches for appearance-based object\ndiscovery and find that a novel form of spatio-temporal super-pixels gives the\nhighest quality candidate object models in terms of precision and recall. Live\nexperiments with a Baxter robot demonstrate a holistic pipeline capable of\nautomatic discovery, verification, detection, tracking and reconstruction of\nunknown objects.\n"}}], "languages": [null], "subjects": ["computer science - robotics", "computer science - computer vision and pattern recognition"], "providerUpdatedDateTime": "2014-11-05T00:00:00", "uris": {"canonicalUri": "http://arxiv.org/abs/1411.0802"}}, {"publisher": {"name": ""}, "description": "  In contrast to today's IP-based host-oriented Internet architecture,\nInformation-Centric Networking (ICN) emphasizes content by making it directly\naddressable and routable. Named Data Networking (NDN) architecture is an\ninstance of ICN that is being developed as a candidate next-generation Internet\narchitecture. By opportunistically caching content within the network (in\nrouters), NDN appears to be well-suited for large-scale content distribution\nand for meeting the needs of increasingly mobile and bandwidth-hungry\napplications that dominate today's Internet.\n  One key feature of NDN is the requirement for each content object to be\ndigitally signed by its producer. Thus, NDN should be, in principle, immune to\ndistributing fake (aka \"poisoned\") content. However, in practice, this poses\ntwo challenges for detecting fake content in NDN routers: (1) overhead due to\nsignature verification and certificate chain traversal, and (2) lack of trust\ncontext, i.e., determining which public keys are trusted to verify which\ncontent. Because of these issues, NDN does not force routers to verify content\nsignatures, which makes the architecture susceptible to content poisoning\nattacks.\n  This paper explores root causes of, and some cures for, content poisoning\nattacks in NDN. In the process, it becomes apparent that meaningful mitigation\nof content poisoning is contingent upon a network-layer trust management\narchitecture, elements of which we construct while carefully justifying\nspecific design choices. This work represents the initial effort towards\ncomprehensive trust management for NDN.\n", "contributors": [{"name": "Ghali, Cesar", "sameAs": [], "familyName": "Ghali", "additionalName": "", "givenName": "Cesar", "email": ""}, {"name": "Tsudik, Gene", "sameAs": [], "familyName": "Tsudik", "additionalName": "", "givenName": "Gene", "email": ""}, {"name": "Uzun, Ersin", "sameAs": [], "familyName": "Uzun", "additionalName": "", "givenName": "Ersin", "email": ""}], "title": "Elements of Trust in Named-Data Networking", "shareProperties": {"source": "arxiv_oai"}, "otherProperties": [{"name": "type", "properties": {"type": "text"}}, {"name": "format", "properties": {"format": []}}, {"name": "date", "properties": {"date": ["2014-02-13", "2014-10-30"]}}, {"name": "identifier", "properties": {"identifier": ["http://arxiv.org/abs/1402.3332", "ACM SIGCOMM Computer Communication Review, Volume 44 Issue 5,\n  October 2014", "doi:10.1145/2677046.2677049", "oai:arXiv.org:1402.3332"]}}, {"name": "setSpec", "properties": {"setSpec": "cs"}}, {"name": "description", "properties": {"description": ["  In contrast to today's IP-based host-oriented Internet architecture,\nInformation-Centric Networking (ICN) emphasizes content by making it directly\naddressable and routable. Named Data Networking (NDN) architecture is an\ninstance of ICN that is being developed as a candidate next-generation Internet\narchitecture. By opportunistically caching content within the network (in\nrouters), NDN appears to be well-suited for large-scale content distribution\nand for meeting the needs of increasingly mobile and bandwidth-hungry\napplications that dominate today's Internet.\n  One key feature of NDN is the requirement for each content object to be\ndigitally signed by its producer. Thus, NDN should be, in principle, immune to\ndistributing fake (aka \"poisoned\") content. However, in practice, this poses\ntwo challenges for detecting fake content in NDN routers: (1) overhead due to\nsignature verification and certificate chain traversal, and (2) lack of trust\ncontext, i.e., determining which public keys are trusted to verify which\ncontent. Because of these issues, NDN does not force routers to verify content\nsignatures, which makes the architecture susceptible to content poisoning\nattacks.\n  This paper explores root causes of, and some cures for, content poisoning\nattacks in NDN. In the process, it becomes apparent that meaningful mitigation\nof content poisoning is contingent upon a network-layer trust management\narchitecture, elements of which we construct while carefully justifying\nspecific design choices. This work represents the initial effort towards\ncomprehensive trust management for NDN.\n", "Comment: 9 pages, 2 figures"]}}], "languages": [null], "subjects": ["computer science - cryptography and security", "computer science - networking and internet architecture"], "providerUpdatedDateTime": "2014-10-31T00:00:00", "uris": {"canonicalUri": "http://arxiv.org/abs/1402.3332"}}, {"publisher": {"name": ""}, "description": "  We consider channel/subspace tracking systems for temporally correlated\nmillimeter wave (e.g., E-band) multiple-input multiple-output (MIMO) channels.\nOur focus is given to the tracking algorithm in the non-line-of-sight (NLoS)\nenvironment, where the transmitter and the receiver are equipped with hybrid\nanalog/digital precoder and combiner, respectively. In the absence of\nstraightforward time-correlated channel model in the millimeter wave MIMO\nliterature, we present a temporal MIMO channel evolution model for NLoS\nmillimeter wave scenarios. Considering that conventional MIMO channel tracking\nalgorithms in microwave bands are not directly applicable, we propose a new\nchannel tracking technique based on sequentially updating the precoder and\ncombiner. Numerical results demonstrate the superior channel tracking ability\nof the proposed technique over independent sounding approach in the presented\nchannel model and the spatial channel model (SCM) adopted in 3GPP\nspecification.\n", "contributors": [{"name": "He, Jiguang", "sameAs": [], "familyName": "He", "additionalName": "", "givenName": "Jiguang", "email": ""}, {"name": "Kim, Taejoon", "sameAs": [], "familyName": "Kim", "additionalName": "", "givenName": "Taejoon", "email": ""}, {"name": "Ghauch, Hadi", "sameAs": [], "familyName": "Ghauch", "additionalName": "", "givenName": "Hadi", "email": ""}, {"name": "Liu, Kunpeng", "sameAs": [], "familyName": "Liu", "additionalName": "", "givenName": "Kunpeng", "email": ""}, {"name": "Wang, Guangjian", "sameAs": [], "familyName": "Wang", "additionalName": "", "givenName": "Guangjian", "email": ""}], "title": "Millimeter Wave MIMO Channel Tracking Systems", "shareProperties": {"source": "arxiv_oai"}, "otherProperties": [{"name": "type", "properties": {"type": "text"}}, {"name": "format", "properties": {"format": []}}, {"name": "date", "properties": {"date": "2014-12-13"}}, {"name": "identifier", "properties": {"identifier": ["http://arxiv.org/abs/1412.4224", "oai:arXiv.org:1412.4224"]}}, {"name": "setSpec", "properties": {"setSpec": ["cs", "math"]}}, {"name": "description", "properties": {"description": ["  We consider channel/subspace tracking systems for temporally correlated\nmillimeter wave (e.g., E-band) multiple-input multiple-output (MIMO) channels.\nOur focus is given to the tracking algorithm in the non-line-of-sight (NLoS)\nenvironment, where the transmitter and the receiver are equipped with hybrid\nanalog/digital precoder and combiner, respectively. In the absence of\nstraightforward time-correlated channel model in the millimeter wave MIMO\nliterature, we present a temporal MIMO channel evolution model for NLoS\nmillimeter wave scenarios. Considering that conventional MIMO channel tracking\nalgorithms in microwave bands are not directly applicable, we propose a new\nchannel tracking technique based on sequentially updating the precoder and\ncombiner. Numerical results demonstrate the superior channel tracking ability\nof the proposed technique over independent sounding approach in the presented\nchannel model and the spatial channel model (SCM) adopted in 3GPP\nspecification.\n", "Comment: 6 pages, 3 figures, conference"]}}], "languages": [null], "subjects": ["computer science - information theory"], "providerUpdatedDateTime": "2014-12-16T00:00:00", "uris": {"canonicalUri": "http://arxiv.org/abs/1412.4224"}}, {"publisher": {"name": "Massachusetts Institute of Technology"}, "description": "One of the primary problems in constructing risk-stratification models for medical applications is that the data are often noisy, incomplete, and suffer from high class-imbalance. This problem becomes more severe when the total amount of data relevant to the task of interest is small. We address this problem in the context of risk-stratifying patients receiving isolated surgical aortic valve replacements (isolated AVR) for the adverse outcomes of operative mortality and stroke. We work with data from two hospitals (Hospital 1 and Hospital 2) in the Society of Thoracic Surgeons (STS) Adult Cardiac Surgery Database. Because the data available for our application of interest (target data) are limited, developing an accurate model using only these data is infeasible. Instead, we investigate transfer learning approaches to utilize data from other cardiac surgery procedures as well as from other institutions (source data). We first evaluate the effectiveness of leveraging information across procedures within a single hospital. We achieve significant improvements over baseline: at Hospital 1, the average AUC for operative mortality increased from 0.58 to 0.70. However, not all source examples are equally useful. Next, we evaluate the effectiveness of leveraging data across hospitals. We show that leveraging information across hospitals has variable utility; although it can result in worse performance (average AUC for stroke at Hospital 1 dropped from 0.61 to 0.56), it can also lead to significant improvements (average AUC for operative mortality at Hospital 1 increased from 0.70 to 0.72). Finally, we present an automated approach to leveraging the available source data. We investigate how removing source data based on how far they are from the mean of the target data affects performance. We propose an instance-weighting scheme based on these distances. This automated instance-weighting approach can achieve small, but significant improvements over using all of the data without weights (average AUC for operative mortality at Hospital 1 increased from 0.72 to 0.73). Research on these methods can have an important impact on the development of clinical risk-stratification tools targeted towards specific patient populations.", "contributors": [{"name": "Gong, Jen J. (Jen Jian)", "sameAs": [], "familyName": "Gong", "additionalName": "J.", "givenName": "Jen", "email": ""}, {"name": "Massachusetts Institute of Technology. Department of Electrical Engineering and Computer Science.", "sameAs": [], "familyName": "Science.", "additionalName": "Institute of Technology. Department of Electrical Engineering and Computer", "givenName": "Massachusetts", "email": ""}, {"name": "John V. Guttag.", "sameAs": [], "familyName": "Guttag.", "additionalName": "V.", "givenName": "John", "email": ""}], "title": "Improving clinical risk-stratification tools : instance-transfer for selecting relevant training data", "shareProperties": {"source": "mit"}, "otherProperties": [{"name": "type", "properties": {"type": "Thesis"}}, {"name": "source", "properties": {"source": []}}, {"name": "format", "properties": {"format": "71 pages"}}, {"name": "rights", "properties": {"rights": ["M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.", "http://dspace.mit.edu/handle/1721.1/7582"]}}, {"name": "identifier", "properties": {"identifier": ["http://hdl.handle.net/1721.1/91090", "892724540", "oai:dspace.mit.edu:1721.1/91090"]}}, {"name": "relation", "properties": {"relation": []}}, {"name": "date", "properties": {"date": ["2014-10-21T17:25:32Z", "2014-10-21T17:25:32Z", "2014", "2014"]}}, {"name": "description", "properties": {"description": ["One of the primary problems in constructing risk-stratification models for medical applications is that the data are often noisy, incomplete, and suffer from high class-imbalance. This problem becomes more severe when the total amount of data relevant to the task of interest is small. We address this problem in the context of risk-stratifying patients receiving isolated surgical aortic valve replacements (isolated AVR) for the adverse outcomes of operative mortality and stroke. We work with data from two hospitals (Hospital 1 and Hospital 2) in the Society of Thoracic Surgeons (STS) Adult Cardiac Surgery Database. Because the data available for our application of interest (target data) are limited, developing an accurate model using only these data is infeasible. Instead, we investigate transfer learning approaches to utilize data from other cardiac surgery procedures as well as from other institutions (source data). We first evaluate the effectiveness of leveraging information across procedures within a single hospital. We achieve significant improvements over baseline: at Hospital 1, the average AUC for operative mortality increased from 0.58 to 0.70. However, not all source examples are equally useful. Next, we evaluate the effectiveness of leveraging data across hospitals. We show that leveraging information across hospitals has variable utility; although it can result in worse performance (average AUC for stroke at Hospital 1 dropped from 0.61 to 0.56), it can also lead to significant improvements (average AUC for operative mortality at Hospital 1 increased from 0.70 to 0.72). Finally, we present an automated approach to leveraging the available source data. We investigate how removing source data based on how far they are from the mean of the target data affects performance. We propose an instance-weighting scheme based on these distances. This automated instance-weighting approach can achieve small, but significant improvements over using all of the data without weights (average AUC for operative mortality at Hospital 1 increased from 0.72 to 0.73). Research on these methods can have an important impact on the development of clinical risk-stratification tools targeted towards specific patient populations.", "by Jen J. Gong.", "Thesis: S.M. in Computer Science and Engineering, Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2014.", "52", "Cataloged from PDF version of thesis.", "Includes bibliographical references (pages 66-71)."]}}, {"name": "setSpec", "properties": {"setSpec": ["hdl_1721.1_7817", "hdl_1721.1_7663"]}}], "languages": [null], "subjects": ["electrical engineering and computer science."], "providerUpdatedDateTime": "2015-04-09T06:21:41", "uris": {"canonicalUri": "http://hdl.handle.net/1721.1/91090"}}, {"publisher": {"name": ""}, "description": "  TCP and its variants have suffered from surprisingly poor performance for\ndecades. We argue the TCP family has little hope to achieve consistent high\nperformance due to a fundamental architectural deficiency: hardwiring\npacket-level events to control responses without understanding the real\nperformance result of its actions. We propose Performance-oriented Congestion\nControl (PCC), a new congestion control architecture in which each sender\ncontinuously observes the connection between its actions and empirically\nexperienced performance, enabling it to consistently adopt actions that result\nin high performance. We prove that PCC converges to a stable and fair\nequilibrium. Across many real-world and challenging environments, PCC shows\nconsistent and often 10x performance improvement, with better fairness and\nstability than TCP. PCC requires no router hardware support or new packet\nformat.\n", "contributors": [{"name": "Dong, Mo", "sameAs": [], "familyName": "Dong", "additionalName": "", "givenName": "Mo", "email": ""}, {"name": "Li, Qingxi", "sameAs": [], "familyName": "Li", "additionalName": "", "givenName": "Qingxi", "email": ""}, {"name": "Zarchy, Doron", "sameAs": [], "familyName": "Zarchy", "additionalName": "", "givenName": "Doron", "email": ""}, {"name": "Godfrey, Brighten", "sameAs": [], "familyName": "Godfrey", "additionalName": "", "givenName": "Brighten", "email": ""}, {"name": "Schapira, Michael", "sameAs": [], "familyName": "Schapira", "additionalName": "", "givenName": "Michael", "email": ""}], "title": "PCC: Re-architecting Congestion Control for Consistent High Performance", "shareProperties": {"source": "arxiv_oai"}, "otherProperties": [{"name": "type", "properties": {"type": "text"}}, {"name": "format", "properties": {"format": []}}, {"name": "date", "properties": {"date": ["2014-09-24", "2014-09-30"]}}, {"name": "identifier", "properties": {"identifier": ["http://arxiv.org/abs/1409.7092", "oai:arXiv.org:1409.7092"]}}, {"name": "setSpec", "properties": {"setSpec": "cs"}}, {"name": "description", "properties": {"description": "  TCP and its variants have suffered from surprisingly poor performance for\ndecades. We argue the TCP family has little hope to achieve consistent high\nperformance due to a fundamental architectural deficiency: hardwiring\npacket-level events to control responses without understanding the real\nperformance result of its actions. We propose Performance-oriented Congestion\nControl (PCC), a new congestion control architecture in which each sender\ncontinuously observes the connection between its actions and empirically\nexperienced performance, enabling it to consistently adopt actions that result\nin high performance. We prove that PCC converges to a stable and fair\nequilibrium. Across many real-world and challenging environments, PCC shows\nconsistent and often 10x performance improvement, with better fairness and\nstability than TCP. PCC requires no router hardware support or new packet\nformat.\n"}}], "languages": [null], "subjects": ["computer science - networking and internet architecture"], "providerUpdatedDateTime": "2014-10-07T00:00:00", "uris": {"canonicalUri": "http://arxiv.org/abs/1409.7092"}}, {"publisher": {"name": ""}, "description": "  Mean Field Variational Bayes (MFVB) is a popular posterior approximation\nmethod due to its fast runtime on large-scale data sets. However, it is well\nknown that a major failing of MFVB is its (sometimes severe) underestimates of\nthe uncertainty of model variables and lack of information about model variable\ncovariance. We develop a fast, general methodology for exponential families\nthat augments MFVB to deliver accurate uncertainty estimates for model\nvariables -- both for individual variables and coherently across variables.\nMFVB for exponential families defines a fixed-point equation in the means of\nthe approximating posterior, and our approach yields a covariance estimate by\nperturbing this fixed point. Inspired by linear response theory, we call our\nmethod linear response variational Bayes (LRVB). We demonstrate the accuracy of\nour method on simulated data sets.\n", "contributors": [{"name": "Giordano, Ryan", "sameAs": [], "familyName": "Giordano", "additionalName": "", "givenName": "Ryan", "email": ""}, {"name": "Broderick, Tamara", "sameAs": [], "familyName": "Broderick", "additionalName": "", "givenName": "Tamara", "email": ""}], "title": "Covariance Matrices for Mean Field Variational Bayes", "shareProperties": {"source": "arxiv_oai"}, "otherProperties": [{"name": "type", "properties": {"type": "text"}}, {"name": "format", "properties": {"format": []}}, {"name": "date", "properties": {"date": "2014-10-24"}}, {"name": "identifier", "properties": {"identifier": ["http://arxiv.org/abs/1410.6853", "oai:arXiv.org:1410.6853"]}}, {"name": "setSpec", "properties": {"setSpec": ["cs", "stat"]}}, {"name": "description", "properties": {"description": ["  Mean Field Variational Bayes (MFVB) is a popular posterior approximation\nmethod due to its fast runtime on large-scale data sets. However, it is well\nknown that a major failing of MFVB is its (sometimes severe) underestimates of\nthe uncertainty of model variables and lack of information about model variable\ncovariance. We develop a fast, general methodology for exponential families\nthat augments MFVB to deliver accurate uncertainty estimates for model\nvariables -- both for individual variables and coherently across variables.\nMFVB for exponential families defines a fixed-point equation in the means of\nthe approximating posterior, and our approach yields a covariance estimate by\nperturbing this fixed point. Inspired by linear response theory, we call our\nmethod linear response variational Bayes (LRVB). We demonstrate the accuracy of\nour method on simulated data sets.\n", "Comment: 12 pages, 2 figures"]}}], "languages": [null], "subjects": ["computer science - learning", "statistics - machine learning", "statistics - methodology"], "providerUpdatedDateTime": "2014-10-28T00:00:00", "uris": {"canonicalUri": "http://arxiv.org/abs/1410.6853"}}, {"publisher": {"name": ""}, "description": "  Based on the axiomatization of reversible computing RACP, we generalize it to\nquantum reversible computing which is called qRACP. By use of the framework of\nquantum configuration, we show that structural reversibility and quantum state\nreversibility must be satisfied simultaneously in quantum reversible\ncomputation. RACP and qRACP has the same axiomatization modulo the so-called\nquantum forward-reverse bisimularity, that is, classical reversible computing\nand quantum reversible computing are unified.\n", "contributors": [{"name": "Wang, Yong", "sameAs": [], "familyName": "Wang", "additionalName": "", "givenName": "Yong", "email": ""}], "title": "An Algebra of Reversible Quantum Computing", "shareProperties": {"source": "arxiv_oai"}, "otherProperties": [{"name": "type", "properties": {"type": "text"}}, {"name": "format", "properties": {"format": []}}, {"name": "date", "properties": {"date": "2015-01-16"}}, {"name": "identifier", "properties": {"identifier": ["http://arxiv.org/abs/1501.05260", "oai:arXiv.org:1501.05260"]}}, {"name": "setSpec", "properties": {"setSpec": "cs"}}, {"name": "description", "properties": {"description": ["  Based on the axiomatization of reversible computing RACP, we generalize it to\nquantum reversible computing which is called qRACP. By use of the framework of\nquantum configuration, we show that structural reversibility and quantum state\nreversibility must be satisfied simultaneously in quantum reversible\ncomputation. RACP and qRACP has the same axiomatization modulo the so-called\nquantum forward-reverse bisimularity, that is, classical reversible computing\nand quantum reversible computing are unified.\n", "Comment: arXiv admin note: substantial text overlap with arXiv:1311.2960,\n  arXiv:1410.5131, arXiv:1312.0686, arXiv:1404.0665"]}}], "languages": [null], "subjects": ["computer science - logic in computer science"], "providerUpdatedDateTime": "2015-01-22T00:00:00", "uris": {"canonicalUri": "http://arxiv.org/abs/1501.05260"}}, {"publisher": {"name": ""}, "description": "  Distributed Opportunistic Scheduling (DOS) is inherently harder than\nconventional opportunistic scheduling due to the absence of a central entity\nthat has knowledge of all the channel states. With DOS, stations contend for\nthe channel using random access; after a successful contention, they measure\nthe channel conditions and only transmit in case of a good channel, while\ngiving up the transmission opportunity when the channel conditions are poor.\nThe distributed nature of DOS systems makes them vulnerable to selfish users:\nby deviating from the protocol and using more transmission opportunities, a\nselfish user can gain a greater share of the wireless resources at the expense\nof the well-behaved users. In this paper, we address the selfishness problem in\nDOS from a game theoretic standpoint. We propose an algorithm that satisfies\nthe following properties: (i) when all stations implement the algorithm, the\nwireless network is driven to the optimal point of operation, and (ii) one or\nmore selfish stations cannot gain any profit by deviating from the algorithm.\nThe key idea of the algorithm is to react to a selfish station by using a more\naggressive configuration that (indirectly) punishes this station. We build on\nmultivariable control theory to design a mechanism for punishment that on the\none hand is sufficiently severe to prevent selfish behavior while on the other\nhand is light enough to guarantee that, in the absence of selfish behavior, the\nsystem is stable and converges to the optimum point of operation. We conduct a\ngame theoretic analysis based on repeated games to show the algorithm's\neffectiveness against selfish stations. These results are confirmed by\nextensive simulations.\n", "contributors": [{"name": "Banchs, Albert", "sameAs": [], "familyName": "Banchs", "additionalName": "", "givenName": "Albert", "email": ""}, {"name": "Garcia-Saavedra, Andres", "sameAs": [], "familyName": "Garcia-Saavedra", "additionalName": "", "givenName": "Andres", "email": ""}, {"name": "Serrano, Pablo", "sameAs": [], "familyName": "Serrano", "additionalName": "", "givenName": "Pablo", "email": ""}, {"name": "Widmer, Joerg", "sameAs": [], "familyName": "Widmer", "additionalName": "", "givenName": "Joerg", "email": ""}], "title": "A Game Theoretic Approach to Distributed Opportunistic Scheduling", "shareProperties": {"source": "arxiv_oai"}, "otherProperties": [{"name": "type", "properties": {"type": "text"}}, {"name": "format", "properties": {"format": []}}, {"name": "date", "properties": {"date": "2011-07-22"}}, {"name": "identifier", "properties": {"identifier": ["http://arxiv.org/abs/1107.4452", "oai:arXiv.org:1107.4452"]}}, {"name": "setSpec", "properties": {"setSpec": "cs"}}, {"name": "description", "properties": {"description": "  Distributed Opportunistic Scheduling (DOS) is inherently harder than\nconventional opportunistic scheduling due to the absence of a central entity\nthat has knowledge of all the channel states. With DOS, stations contend for\nthe channel using random access; after a successful contention, they measure\nthe channel conditions and only transmit in case of a good channel, while\ngiving up the transmission opportunity when the channel conditions are poor.\nThe distributed nature of DOS systems makes them vulnerable to selfish users:\nby deviating from the protocol and using more transmission opportunities, a\nselfish user can gain a greater share of the wireless resources at the expense\nof the well-behaved users. In this paper, we address the selfishness problem in\nDOS from a game theoretic standpoint. We propose an algorithm that satisfies\nthe following properties: (i) when all stations implement the algorithm, the\nwireless network is driven to the optimal point of operation, and (ii) one or\nmore selfish stations cannot gain any profit by deviating from the algorithm.\nThe key idea of the algorithm is to react to a selfish station by using a more\naggressive configuration that (indirectly) punishes this station. We build on\nmultivariable control theory to design a mechanism for punishment that on the\none hand is sufficiently severe to prevent selfish behavior while on the other\nhand is light enough to guarantee that, in the absence of selfish behavior, the\nsystem is stable and converges to the optimum point of operation. We conduct a\ngame theoretic analysis based on repeated games to show the algorithm's\neffectiveness against selfish stations. These results are confirmed by\nextensive simulations.\n"}}], "languages": [null], "subjects": ["computer science - networking and internet architecture"], "providerUpdatedDateTime": "2015-03-19T00:00:00", "uris": {"canonicalUri": "http://arxiv.org/abs/1107.4452"}}, {"publisher": {"name": ""}, "description": "  Security metrics serve as a powerful tool for organizations to understand the\neffectiveness of protecting computer networks. However majority of these\nmeasurement techniques don't adequately help corporations to make informed risk\nmanagement decisions. In this paper we present a stochastic security framework\nfor obtaining quantitative measures of security by taking into account the\ndynamic attributes associated with vulnerabilities that can change over time.\nOur model is novel as existing research in attack graph analysis do not\nconsider the temporal aspects associated with the vulnerabilities, such as the\navailability of exploits and patches which can affect the overall network\nsecurity based on how the vulnerabilities are interconnected and leveraged to\ncompromise the system. In order to have a more realistic representation of how\nthe security state of the network would vary over time, a nonhomogeneous model\nis developed which incorporates a time dependent covariate, namely the\nvulnerability age. The daily transition-probability matrices are estimated\nusing Frei's Vulnerability Lifecycle model. We also leverage the trusted CVSS\nmetric domain to analyze how the total exploitability and impact measures\nevolve over a time period for a given network.\n", "contributors": [{"name": "Abraham, Subil", "sameAs": [], "familyName": "Abraham", "additionalName": "", "givenName": "Subil", "email": ""}, {"name": "Nair, Suku", "sameAs": [], "familyName": "Nair", "additionalName": "", "givenName": "Suku", "email": ""}], "title": "A Predictive Framework for Cyber Security Analytics using Attack Graphs", "shareProperties": {"source": "arxiv_oai"}, "otherProperties": [{"name": "type", "properties": {"type": "text"}}, {"name": "format", "properties": {"format": []}}, {"name": "date", "properties": {"date": "2015-02-04"}}, {"name": "identifier", "properties": {"identifier": ["http://arxiv.org/abs/1502.01240", "oai:arXiv.org:1502.01240"]}}, {"name": "setSpec", "properties": {"setSpec": "cs"}}, {"name": "description", "properties": {"description": ["  Security metrics serve as a powerful tool for organizations to understand the\neffectiveness of protecting computer networks. However majority of these\nmeasurement techniques don't adequately help corporations to make informed risk\nmanagement decisions. In this paper we present a stochastic security framework\nfor obtaining quantitative measures of security by taking into account the\ndynamic attributes associated with vulnerabilities that can change over time.\nOur model is novel as existing research in attack graph analysis do not\nconsider the temporal aspects associated with the vulnerabilities, such as the\navailability of exploits and patches which can affect the overall network\nsecurity based on how the vulnerabilities are interconnected and leveraged to\ncompromise the system. In order to have a more realistic representation of how\nthe security state of the network would vary over time, a nonhomogeneous model\nis developed which incorporates a time dependent covariate, namely the\nvulnerability age. The daily transition-probability matrices are estimated\nusing Frei's Vulnerability Lifecycle model. We also leverage the trusted CVSS\nmetric domain to analyze how the total exploitability and impact measures\nevolve over a time period for a given network.\n", "Comment: 17 pages, 8 figures in International Journal of Computer Networks &\n  Communications (IJCNC) January 2015. ISSN:0974-9322 [Online]; 0975-2293\n  [Print]"]}}], "languages": [null], "subjects": ["computer science - cryptography and security"], "providerUpdatedDateTime": "2015-02-05T00:00:00", "uris": {"canonicalUri": "http://arxiv.org/abs/1502.01240"}}, {"publisher": {"name": ""}, "description": "  Network control refers to a very large and diverse set of problems including\ncontrollability of linear time-invariant dynamical systems evolving over time\nthat have inputs and outputs. The network control problem in this setting is to\nselect the appropriate input to steer the network into a desired output state.\nExamples of the output state include the throughput of a communications\nnetwork, transcription factor concentration in a gene regulatory network,\ncustomer purchases in a marketing context subject to social influences and the\namount of flux flowing through a biochemical network.\n  We focus on control of linear dynamical systems under the notion of\nstructural controllability which is intimately connected to finding maximum\nmatchings. Hence, a natural objective is studying scalable and fast algorithms\nfor this task. We first show the convergence of matching algorithms for\ndifferent random networks and then analyze a popular, fast and practical\nheuristic due to Karp and Sipser. We establish the optimality of both the\nKarp-Sipser Algorithm as well as a simplification of it, and provide results\nconcerning the asymptotic size of maximum matchings for an extensive class of\nrandom networks.\n", "contributors": [{"name": "Faradonbeh, Mohamad Kazem Shirani", "sameAs": [], "familyName": "Faradonbeh", "additionalName": "Kazem Shirani", "givenName": "Mohamad", "email": ""}, {"name": "Tewari, Ambuj", "sameAs": [], "familyName": "Tewari", "additionalName": "", "givenName": "Ambuj", "email": ""}, {"name": "Michailidis, George", "sameAs": [], "familyName": "Michailidis", "additionalName": "", "givenName": "George", "email": ""}], "title": "Optimality of Fast Matching Algorithms for Random Networks with\n  Applications to Structural Controllability", "shareProperties": {"source": "arxiv_oai"}, "otherProperties": [{"name": "type", "properties": {"type": "text"}}, {"name": "format", "properties": {"format": []}}, {"name": "date", "properties": {"date": "2015-03-27"}}, {"name": "identifier", "properties": {"identifier": ["http://arxiv.org/abs/1503.08019", "oai:arXiv.org:1503.08019"]}}, {"name": "setSpec", "properties": {"setSpec": ["cs", "stat"]}}, {"name": "description", "properties": {"description": "  Network control refers to a very large and diverse set of problems including\ncontrollability of linear time-invariant dynamical systems evolving over time\nthat have inputs and outputs. The network control problem in this setting is to\nselect the appropriate input to steer the network into a desired output state.\nExamples of the output state include the throughput of a communications\nnetwork, transcription factor concentration in a gene regulatory network,\ncustomer purchases in a marketing context subject to social influences and the\namount of flux flowing through a biochemical network.\n  We focus on control of linear dynamical systems under the notion of\nstructural controllability which is intimately connected to finding maximum\nmatchings. Hence, a natural objective is studying scalable and fast algorithms\nfor this task. We first show the convergence of matching algorithms for\ndifferent random networks and then analyze a popular, fast and practical\nheuristic due to Karp and Sipser. We establish the optimality of both the\nKarp-Sipser Algorithm as well as a simplification of it, and provide results\nconcerning the asymptotic size of maximum matchings for an extensive class of\nrandom networks.\n"}}], "languages": [null], "subjects": ["computer science - data structures and algorithms", "computer science - systems and control", "statistics - other statistics"], "providerUpdatedDateTime": "2015-03-30T00:00:00", "uris": {"canonicalUri": "http://arxiv.org/abs/1503.08019"}}], "time": 0.02}